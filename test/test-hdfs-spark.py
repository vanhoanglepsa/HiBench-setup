#!/usr/bin/env python3
"""
Script test Ä‘Æ¡n giáº£n: Äá»c file tá»« HDFS báº±ng Spark vÃ  thá»±c hiá»‡n phÃ¢n tÃ­ch
Sá»­ dá»¥ng DataFrame API Ä‘á»ƒ trÃ¡nh serialization issues
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, length, split, explode, lower, count as sql_count
import sys

def main():
    print("=" * 60)
    print("ğŸš€ SPARK + HDFS TEST")
    print("=" * 60)
    print()
    
    # Táº¡o Spark Session
    print("ğŸ“Š Khá»Ÿi táº¡o Spark Session...")
    spark = SparkSession.builder \
        .appName("HDFS-Spark-Test") \
        .master("spark://spark-master:7077") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    
    print("âœ… Spark Session Ä‘Ã£ sáºµn sÃ ng!")
    print(f"   - Spark Version: {spark.version}")
    print(f"   - Master: {spark.sparkContext.master}")
    print()
    
    # ÄÆ°á»ng dáº«n file trÃªn HDFS
    hdfs_path = "hdfs://namenode:9000/test/sample-data.txt"
    
    print(f"ğŸ“ Äá»c file tá»« HDFS: {hdfs_path}")
    
    try:
        # Äá»c file tá»« HDFS as DataFrame
        df = spark.read.text(hdfs_path)
        
        print("âœ… Äá»c file thÃ nh cÃ´ng!")
        print()
        
        # 1. Äáº¿m sá»‘ dÃ²ng
        line_count = df.count()
        print(f"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {line_count}")
        print()
        
        # 2. Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u tiÃªn
        print("ğŸ“‹ 5 dÃ²ng Ä‘áº§u tiÃªn:")
        print("-" * 60)
        first_lines = df.take(5)
        for i, row in enumerate(first_lines, 1):
            print(f"  {i}. {row.value}")
        print("-" * 60)
        print()
        
        # 3. Word Count vá»›i DataFrame API
        print("ğŸ”¤ PhÃ¢n tÃ­ch tá»« khÃ³a:")
        
        # Split thÃ nh words
        words_df = df.select(explode(split(lower(col("value")), "\\s+")).alias("word"))
        
        # Lá»c bá» cÃ¡c string rá»—ng
        words_df = words_df.filter(col("word") != "")
        
        # Äáº¿m tá»•ng sá»‘ tá»«
        total_words = words_df.count()
        print(f"   - Tá»•ng sá»‘ tá»«: {total_words}")
        
        # Äáº¿m frequency vÃ  láº¥y top 10
        word_freq = words_df.groupBy("word").agg(sql_count("*").alias("count"))
        top_words = word_freq.orderBy(col("count").desc()).take(10)
        
        print("   - Top 10 tá»« xuáº¥t hiá»‡n nhiá»u nháº¥t:")
        for row in top_words:
            print(f"     â€¢ {row.word}: {row['count']} láº§n")
        print()
        
        # 4. TÃ¬m cÃ¡c dÃ²ng chá»©a tá»« "Spark"
        spark_lines = df.filter(col("value").contains("Spark") | col("value").contains("spark"))
        spark_count = spark_lines.count()
        print(f"ğŸ” TÃ¬m tháº¥y {spark_count} dÃ²ng chá»©a tá»« 'Spark':")
        print("-" * 60)
        for i, row in enumerate(spark_lines.collect(), 1):
            print(f"  {i}. {row.value}")
        print("-" * 60)
        print()
        
        # 5. Statistics
        print("ğŸ“Š Thá»‘ng kÃª:")
        
        # TÃ­nh Ä‘á»™ dÃ i má»—i dÃ²ng
        df_with_length = df.withColumn("line_length", length(col("value")))
        
        # Aggregate statistics
        stats = df_with_length.agg(
            sql_count("*").alias("total_lines"),
            sql_count("line_length").alias("total_chars_sum")
        ).collect()[0]
        
        # Láº¥y min, max, avg
        length_stats = df_with_length.select("line_length").describe().collect()
        
        # Parse results
        for stat in length_stats:
            if stat['summary'] == 'mean':
                avg_length = float(stat['line_length'])
            elif stat['summary'] == 'max':
                max_length = int(float(stat['line_length']))
            elif stat['summary'] == 'min':
                min_length = int(float(stat['line_length']))
        
        # TÃ­nh tá»•ng kÃ½ tá»±
        total_chars = df_with_length.agg({"line_length": "sum"}).collect()[0][0]
        
        print(f"   - Tá»•ng sá»‘ kÃ½ tá»±: {int(total_chars)}")
        print(f"   - Äá»™ dÃ i trung bÃ¬nh má»—i dÃ²ng: {avg_length:.2f} kÃ½ tá»±")
        print(f"   - DÃ²ng dÃ i nháº¥t: {max_length} kÃ½ tá»±")
        print(f"   - DÃ²ng ngáº¯n nháº¥t: {min_length} kÃ½ tá»±")
        print()
        
        print("=" * 60)
        print("âœ… TEST HOÃ€N Táº¤T THÃ€NH CÃ”NG!")
        print("=" * 60)
        print()
        print("ğŸ’¡ Báº¡n cÃ³ thá»ƒ:")
        print("   - Xem Spark UI: http://localhost:4040")
        print("   - Xem HDFS UI: http://localhost:9870")
        print("   - Kiá»ƒm tra file trÃªn HDFS:")
        print(f"     docker exec namenode hdfs dfs -ls /test/")
        print()
        
    except Exception as e:
        print(f"âŒ Lá»–I: {str(e)}")
        import traceback
        traceback.print_exc()
        print()
        print("ğŸ’¡ Kiá»ƒm tra:")
        print("   1. File Ä‘Ã£ Ä‘Æ°á»£c upload lÃªn HDFS chÆ°a?")
        print("   2. HDFS NameNode cÃ³ cháº¡y khÃ´ng?")
        print("   3. Spark cluster cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng?")
        sys.exit(1)
    
    finally:
        # Dá»«ng Spark Session
        spark.stop()

if __name__ == "__main__":
    main()

