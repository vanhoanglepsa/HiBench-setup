#!/bin/bash

# Script to test HDFS + Spark integration
# Not related to HiBench

set -e

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "  ğŸ§ª TEST HDFS + SPARK INTEGRATION"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Check containers
echo "1ï¸âƒ£  Checking containers..."
if ! docker ps | grep -q "spark-master"; then
    echo "âŒ Spark Master is not running!"
    echo "   Run: make start"
    exit 1
fi

if ! docker ps | grep -q "namenode"; then
    echo "âŒ Hadoop NameNode is not running!"
    echo "   Run: make start"
    exit 1
fi

echo "âœ… All containers are running"
echo ""

# Create test directory on HDFS
echo "2ï¸âƒ£  Creating /test/ directory on HDFS..."
docker exec namenode hdfs dfs -mkdir -p /test 2>/dev/null || true
docker exec namenode hdfs dfs -chmod 777 /test
echo "âœ… Directory is ready"
echo ""

# Upload test file to HDFS
echo "3ï¸âƒ£  Uploading test file to HDFS..."
echo "   - File: sample-data.txt"
echo "   - Destination: hdfs://namenode:9000/test/"

# Copy file to container first
docker cp test/sample-data.txt namenode:/tmp/sample-data.txt

# Upload to HDFS
docker exec namenode hdfs dfs -put -f /tmp/sample-data.txt /test/

# Check if file has been uploaded
echo ""
echo "   ğŸ“ Checking file on HDFS:"
docker exec namenode hdfs dfs -ls /test/
echo ""

FILE_SIZE=$(docker exec namenode hdfs dfs -du -h /test/sample-data.txt | awk '{print $1" "$2}')
echo "   âœ… File uploaded successfully! (Size: $FILE_SIZE)"
echo ""

# Copy Python script to Spark container
echo "4ï¸âƒ£  Preparing Spark job..."
docker cp test/test-hdfs-spark.py spark-master:/tmp/test-hdfs-spark.py
echo "âœ… Script is ready"
echo ""

# Run Spark job
echo "5ï¸âƒ£  Running Spark job to read and analyze file..."
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

docker exec spark-master spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --driver-memory 1g \
    --executor-memory 2g \
    --executor-cores 2 \
    /tmp/test-hdfs-spark.py

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ‰ Test complete!"
echo ""
echo "ğŸ“Š You can view:"
echo "   - Spark Master UI:  http://localhost:8080"
echo "   - Spark App UI:     http://localhost:4040"
echo "   - Hadoop HDFS UI:   http://localhost:9870"
echo ""
echo "ğŸ§¹ To clean up test data:"
echo "   docker exec namenode hdfs dfs -rm -r /test"
echo ""

