#!/usr/bin/env python3
"""
HiBench-style WordCount Benchmark
Cháº¡y WordCount trÃªn Spark vá»›i data tá»« HDFS
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, lower, col, count as sql_count
import time
import sys

def main():
    print("=" * 70)
    print("ğŸš€ HIBENCH WORDCOUNT BENCHMARK")
    print("=" * 70)
    print()
    
    # Táº¡o Spark Session
    print("ğŸ“Š Khá»Ÿi táº¡o Spark Session...")
    spark = SparkSession.builder \
        .appName("HiBench-WordCount") \
        .master("spark://spark-master:7077") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.executor.cores", "2") \
        .getOrCreate()
    
    print("âœ… Spark Session sáºµn sÃ ng!")
    print(f"   - Version: {spark.version}")
    print(f"   - Master: {spark.sparkContext.master}")
    print()
    
    # HDFS paths
    input_path = "hdfs://namenode:9000/HiBench/Wordcount/Input"
    output_path = "hdfs://namenode:9000/HiBench/Wordcount/Output"
    
    print(f"ğŸ“ Input: {input_path}")
    print(f"ğŸ“ Output: {output_path}")
    print()
    
    try:
        # Äá»c dá»¯ liá»‡u tá»« HDFS
        print("â³ Äá»c dá»¯ liá»‡u tá»« HDFS...")
        start_time = time.time()
        
        df = spark.read.text(input_path)
        total_lines = df.count()
        
        print(f"âœ… Äá»c thÃ nh cÃ´ng {total_lines} dÃ²ng")
        print()
        
        # WordCount
        print("âš™ï¸  Äang xá»­ lÃ½ WordCount...")
        process_start = time.time()
        
        # Split words vÃ  count
        words_df = df.select(explode(split(lower(col("value")), "\\s+")).alias("word"))
        words_df = words_df.filter(col("word") != "")
        word_counts = words_df.groupBy("word").agg(sql_count("*").alias("count"))
        word_counts = word_counts.orderBy(col("count").desc())
        
        # Write results to HDFS
        print(f"ğŸ’¾ Ghi káº¿t quáº£ vÃ o HDFS: {output_path}")
        word_counts.write.mode("overwrite").csv(output_path)
        
        process_end = time.time()
        
        # Statistics
        total_words = words_df.count()
        unique_words = word_counts.count()
        
        end_time = time.time()
        duration = end_time - start_time
        processing_time = process_end - process_start
        
        print()
        print("=" * 70)
        print("ğŸ“Š Káº¾T QUáº¢ BENCHMARK")
        print("=" * 70)
        print(f"  Tá»•ng dÃ²ng:           {total_lines:,}")
        print(f"  Tá»•ng tá»«:             {total_words:,}")
        print(f"  Tá»« unique:           {unique_words:,}")
        print(f"  Tá»•ng thá»i gian:      {duration:.2f} giÃ¢y")
        print(f"  Thá»i gian xá»­ lÃ½:     {processing_time:.2f} giÃ¢y")
        print(f"  Throughput:          {total_words/duration:,.0f} words/second")
        print("=" * 70)
        print()
        
        # Top 10 words
        print("ğŸ” Top 10 tá»« xuáº¥t hiá»‡n nhiá»u nháº¥t:")
        top_10 = word_counts.take(10)
        for i, row in enumerate(top_10, 1):
            print(f"   {i:2d}. {row.word:20s} : {row['count']:,} láº§n")
        
        print()
        print("âœ… BENCHMARK HOÃ€N Táº¤T!")
        print()
        
    except Exception as e:
        print(f"âŒ Lá»–I: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

