# ðŸ“ Project Structure

## Cáº¥u TrÃºc ThÆ° Má»¥c

```
Set-up/
â”‚
â”œâ”€â”€ ðŸ“„ README.md                    # HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
â”œâ”€â”€ ðŸ“„ QUICKSTART.md                # HÆ°á»›ng dáº«n nhanh 3 phÃºt
â”œâ”€â”€ ðŸ“„ EXAMPLES.md                  # VÃ­ dá»¥ chi tiáº¿t cÃ¡c benchmarks
â”œâ”€â”€ ðŸ“„ TROUBLESHOOTING.md           # Guide kháº¯c phá»¥c lá»—i
â”œâ”€â”€ ðŸ“„ CHANGELOG.md                 # Lá»‹ch sá»­ thay Ä‘á»•i
â”œâ”€â”€ ðŸ“„ PROJECT_STRUCTURE.md         # File nÃ y
â”œâ”€â”€ ðŸ“„ LICENSE                      # Apache License 2.0
â”‚
â”œâ”€â”€ ðŸ³ docker-compose.yml           # Docker orchestration chÃ­nh
â”œâ”€â”€ ðŸ³ Dockerfile                   # Custom Docker image (optional)
â”œâ”€â”€ ðŸ³ .dockerignore                # Ignore files cho Docker build
â”‚
â”œâ”€â”€ ðŸ“ Makefile                     # Automation commands
â”œâ”€â”€ ðŸ™ˆ .gitignore                   # Git ignore patterns
â”‚
â”œâ”€â”€ ðŸ“ config/                      # CÃ¡c file cáº¥u hÃ¬nh
â”‚   â”œâ”€â”€ ðŸ“ hadoop/
â”‚   â”‚   â”œâ”€â”€ core-site.xml          # Hadoop core configuration
â”‚   â”‚   â””â”€â”€ hdfs-site.xml          # HDFS configuration
â”‚   â””â”€â”€ ðŸ“ spark/
â”‚       â”œâ”€â”€ spark-defaults.conf    # Spark default settings
â”‚       â””â”€â”€ spark-env.sh           # Spark environment variables
â”‚
â”œâ”€â”€ ðŸ“ scripts/                     # Automation scripts
â”‚   â”œâ”€â”€ setup.sh                   # â­ Setup ban Ä‘áº§u (main entry point)
â”‚   â”œâ”€â”€ stop.sh                    # Dá»«ng táº¥t cáº£ services
â”‚   â”œâ”€â”€ status.sh                  # Kiá»ƒm tra tráº¡ng thÃ¡i
â”‚   â”œâ”€â”€ init-hdfs.sh               # Khá»Ÿi táº¡o HDFS directories
â”‚   â””â”€â”€ test-wordcount.sh          # Test nhanh WordCount
â”‚
â”œâ”€â”€ ðŸ“ hibench-workspace/           # HiBench configurations
â”‚   â”œâ”€â”€ hibench.conf               # Main HiBench config
â”‚   â”œâ”€â”€ spark.conf                 # Spark-specific config
â”‚   â””â”€â”€ hadoop.conf                # Hadoop-specific config
â”‚
â””â”€â”€ ðŸ“ data/                        # Data directory (mounted to containers)
    â””â”€â”€ .gitkeep                   # Keep directory in git
```

---

## ðŸ“‹ Chi Tiáº¿t Tá»«ng File

### Root Level Files

#### Documentation Files
- **README.md**: TÃ i liá»‡u chÃ­nh, hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ tá»« setup Ä‘áº¿n troubleshooting
- **QUICKSTART.md**: Quick start guide, 3 phÃºt tá»« zero Ä‘áº¿n cháº¡y benchmark Ä‘áº§u tiÃªn
- **EXAMPLES.md**: CÃ¡c vÃ­ dá»¥ chi tiáº¿t, cÃ¡ch cháº¡y tá»«ng loáº¡i benchmark
- **TROUBLESHOOTING.md**: Danh sÃ¡ch cÃ¡c lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch fix
- **CHANGELOG.md**: Lá»‹ch sá»­ versions vÃ  features
- **PROJECT_STRUCTURE.md**: File nÃ y, giáº£i thÃ­ch cáº¥u trÃºc project

#### Configuration Files
- **docker-compose.yml**: 
  - Äá»‹nh nghÄ©a 4 services: namenode, datanode, spark-master, spark-worker
  - Network configuration: hibench-net
  - Volume mounts: hadoop_namenode, hadoop_datanode
  - Port mappings cho Web UIs
  - Health checks

- **Dockerfile**:
  - Base image: openjdk:8-jdk-slim (ARM64 compatible)
  - Install Hadoop 3.2.4
  - Install Spark 3.1.3
  - Clone vÃ  build HiBench
  - Tá»‘i Æ°u cho MacBook M3

- **.dockerignore**: Exclude files khÃ´ng cáº§n thiáº¿t khi build Docker image

#### Automation
- **Makefile**: 
  - Commands: setup, start, stop, restart, status, logs
  - Shell access: shell-spark, shell-hadoop
  - Testing: test, test-quick
  - HDFS operations: hdfs-ls, hdfs-report, hdfs-clean
  - Help: `make help`

- **.gitignore**: Git ignore patterns cho logs, data, IDE files

- **LICENSE**: Apache License 2.0

---

### ðŸ“ config/

Chá»©a cÃ¡c file cáº¥u hÃ¬nh cho Hadoop vÃ  Spark.

#### config/hadoop/
- **core-site.xml**: 
  ```xml
  - fs.defaultFS: hdfs://namenode:9000
  - hadoop.tmp.dir: /tmp/hadoop
  - hadoop.http.staticuser.user: root
  ```

- **hdfs-site.xml**:
  ```xml
  - dfs.replication: 1
  - dfs.permissions.enabled: false (Ä‘á»ƒ Ä‘Æ¡n giáº£n)
  - dfs.webhdfs.enabled: true
  - dfs.namenode.name.dir: /hadoop/dfs/name
  - dfs.datanode.data.dir: /hadoop/dfs/data
  ```

#### config/spark/
- **spark-defaults.conf**:
  ```
  - spark.master: spark://spark-master:7077
  - spark.driver.memory: 1g
  - spark.executor.memory: 2g
  - spark.executor.cores: 2
  - spark.eventLog.enabled: true
  - spark.eventLog.dir: hdfs://namenode:9000/spark-logs
  ```

- **spark-env.sh**:
  ```bash
  - JAVA_HOME, SPARK_MASTER_HOST, SPARK_MASTER_PORT
  - SPARK_WORKER_CORES, SPARK_WORKER_MEMORY
  - HADOOP_CONF_DIR
  ```

---

### ðŸ“ scripts/

Automation scripts Ä‘á»ƒ quáº£n lÃ½ environment.

- **setup.sh** (â­ Main entry point):
  ```bash
  1. Check Docker & Docker Compose
  2. docker-compose up -d
  3. Wait 60s for services to start
  4. Initialize HDFS directories
  5. Print Web UI URLs
  ```

- **stop.sh**:
  ```bash
  - Stop all containers: docker-compose down
  - Option to remove volumes: docker-compose down -v
  ```

- **status.sh**:
  ```bash
  - Show container status: docker-compose ps
  - Show resource usage: docker stats
  - Show HDFS status: hdfs dfsadmin -report
  - List Web UI URLs
  ```

- **init-hdfs.sh**:
  ```bash
  - Create HDFS directories: /HiBench, /spark-logs, /user/root
  - Set permissions: chmod 777
  ```

- **test-wordcount.sh**:
  ```bash
  1. Check containers running
  2. Copy HiBench configs
  3. Prepare WordCount data
  4. Run WordCount benchmark
  5. Show results
  ```

---

### ðŸ“ hibench-workspace/

HiBench configuration files, sáº½ Ä‘Æ°á»£c mount vÃ o containers.

- **hibench.conf** (Main config):
  ```properties
  - hibench.hadoop.home: /opt/hadoop
  - hibench.spark.home: /opt/spark
  - hibench.hdfs.master: hdfs://namenode:9000
  - hibench.spark.master: spark://spark-master:7077
  - hibench.scale.profile: small
  - Data sizes cho cÃ¡c workloads
  ```

- **spark.conf**:
  ```properties
  - Spark-specific settings
  - Executor/driver memory & cores
  - Serialization, network timeouts
  - Event logging
  ```

- **hadoop.conf**:
  ```properties
  - Hadoop paths
  - HDFS master URL
  - MapReduce settings (náº¿u dÃ¹ng)
  - HDFS replication & block size
  ```

---

### ðŸ“ data/

Local data directory, cÃ³ thá»ƒ mount vÃ o containers Ä‘á»ƒ share data.

- **.gitkeep**: Keep empty directory in git

---

## ðŸ”„ Data Flow

```
User
  â”‚
  â”œâ”€> make setup
  â”‚     â””â”€> scripts/setup.sh
  â”‚           â””â”€> docker-compose up -d
  â”‚                 â”œâ”€> namenode container
  â”‚                 â”œâ”€> datanode container
  â”‚                 â”œâ”€> spark-master container
  â”‚                 â””â”€> spark-worker container
  â”‚
  â”œâ”€> make shell-spark
  â”‚     â””â”€> docker exec -it spark-master bash
  â”‚
  â””â”€> Run HiBench
        â”œâ”€> Read configs from /hibench/*.conf
        â”œâ”€> Write data to HDFS (hdfs://namenode:9000/HiBench/)
        â”œâ”€> Submit Spark job to spark-master:7077
        â”œâ”€> Execute on spark-worker
        â””â”€> Write results to /opt/hibench/report/
```

---

## ðŸŒ Network Architecture

```
Docker Network: hibench-net (bridge)
â”‚
â”œâ”€> namenode (9000, 9870)
â”‚     â”œâ”€> HDFS storage
â”‚     â””â”€> Web UI: http://localhost:9870
â”‚
â”œâ”€> datanode
â”‚     â””â”€> HDFS data blocks
â”‚
â”œâ”€> spark-master (7077, 8080, 4040)
â”‚     â”œâ”€> Spark cluster manager
â”‚     â”œâ”€> Master UI: http://localhost:8080
â”‚     â””â”€> App UI: http://localhost:4040
â”‚
â””â”€> spark-worker (8081)
      â”œâ”€> Execute tasks
      â””â”€> Worker UI: http://localhost:8081
```

---

## ðŸ“¦ Docker Volumes

```
hadoop_namenode
  â””â”€> /hadoop/dfs/name (HDFS metadata)

hadoop_datanode
  â””â”€> /hadoop/dfs/data (HDFS data blocks)
```

---

## ðŸš€ Typical Workflow

1. **Setup** (once):
   ```bash
   make setup
   ```

2. **Development**:
   ```bash
   make shell-spark
   cd /opt/hibench
   cp /hibench/*.conf conf/
   ```

3. **Run Benchmark**:
   ```bash
   bin/workloads/micro/wordcount/prepare/prepare.sh
   bin/workloads/micro/wordcount/spark/run.sh
   ```

4. **Check Results**:
   ```bash
   cat report/hibench.report
   # Or Web UI: http://localhost:8080
   ```

5. **Cleanup**:
   ```bash
   make stop
   # or
   make clean  # Remove all data
   ```

---

## ðŸ”§ Customization Points

### Resource Allocation
- **docker-compose.yml**: 
  - `SPARK_WORKER_CORES`
  - `SPARK_WORKER_MEMORY`

### Benchmark Scale
- **hibench-workspace/hibench.conf**:
  - `hibench.scale.profile`
  - Individual workload data sizes

### Spark Tuning
- **hibench-workspace/spark.conf**:
  - Executor/driver memory
  - Parallelism settings
  - Shuffle configuration

### Hadoop Config
- **config/hadoop/*.xml**:
  - HDFS replication
  - Block size
  - Permissions

---

## ðŸ“Š Log Locations

### On Host (via docker-compose logs)
```bash
docker-compose logs namenode
docker-compose logs spark-master
```

### Inside Containers
```
/opt/hadoop/logs/        # Hadoop logs
/opt/spark/logs/         # Spark logs
/opt/hibench/report/     # HiBench results
```

---

**Cáº¥u trÃºc nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ:**
- âœ… Dá»… hiá»ƒu vÃ  maintain
- âœ… TÃ¡ch biá»‡t config vÃ  code
- âœ… Automation tá»‘i Ä‘a
- âœ… Scalable vÃ  customizable
- âœ… Production-ready

**TÃ¬m hiá»ƒu thÃªm**: Xem cÃ¡c file .md trong root directory! ðŸ“š

